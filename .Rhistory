corpus.data$seller
corpus.data[,233]
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, new.data$seller)
corpus.data <- as.data.frame(corpus.data)
corpus.data[,233]
new.data$seller
levels(new.data$seller)
new.data$seller
# Select all "Drugs & Chemicals" ads
matching_vector <- c( str_detect(data$category, "Drugs & Chemicals"))
new.data <- data[matching_vector,]
# Handling : seller
tab_sel <- table(new.data$seller)
tab_sel <- sort(tab_sel, decreasing=TRUE)  # Sorting (biggest in first)
tab_sel <- tab_sel[1:5] # Taking only the most important : main sellers
name_sel <- names(tab_sel)
# New data keeping only the main sellers
new.data <-subset(new.data, seller %in% name_sel)
#new.data$seller <- factor(new.data$seller)
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, new.data$seller)
corpus.data <- as.data.frame(corpus.data)
corpus.data[,233]
new.data$seller
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
View(corpus.data)
View(corpus.data)
new.data$seller
corpus.data <- cbind(corpus.data, new.data$seller)
corpus.data[,233]
View(corpus.data)
class(new.data$seller)
new.data$seller <- as.character(new.data$seller)
corpus.data <- cbind(corpus.data, new.data$seller)
class(new.data$seller)
corpus.data[,233]
View(corpus.data)
corpus.data
corpus.data[,234]
library(tm)
library(plyr)
library(class)
library(caret)
library(stringr)
data <- as.data.frame(read.csv("alphaClean.csv"))
# Select all "Drugs & Chemicals" ads
matching_vector <- c( str_detect(data$category, "Drugs & Chemicals"))
new.data <- data[matching_vector,]
# Handling : seller
tab_sel <- table(new.data$seller)
tab_sel <- sort(tab_sel, decreasing=TRUE)  # Sorting (biggest in first)
tab_sel <- tab_sel[1:5] # Taking only the most important : main sellers
name_sel <- names(tab_sel)
# New data keeping only the main sellers
new.data <-subset(new.data, seller %in% name_sel)
#new.data$seller <- factor(new.data$seller)
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, new.data$seller)
corpus.data <- cbind(corpus.data, new.data$seller)
library(tm)
library(plyr)
library(class)
library(caret)
library(stringr)
data <- as.data.frame(read.csv("alphaClean.csv"))
# Select all "Drugs & Chemicals" ads
matching_vector <- c( str_detect(data$category, "Drugs & Chemicals"))
new.data <- data[matching_vector,]
# Handling : seller
tab_sel <- table(new.data$seller)
tab_sel <- sort(tab_sel, decreasing=TRUE)  # Sorting (biggest in first)
tab_sel <- tab_sel[1:5] # Taking only the most important : main sellers
name_sel <- names(tab_sel)
# New data keeping only the main sellers
new.data <-subset(new.data, seller %in% name_sel)
#new.data$seller <- factor(new.data$seller)
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, new.data$seller)
corpus.data[,234]
corpus.data[,233]
corpus.data[,232]
as.character(new.data$seller)
corpus.data <- cbind(corpus.data, as.character(new.data$seller))
corpus.data[,233]
corpus.data[,234]
corpus.data <- as.data.frame(corpus.data)
View(corpus.data)
View(corpus.data)
corpus.data[,234]
corpus.data[,233]
names(corpus.data[,233])
names(corpus.data)[233]
corpus.data <- subset(corpus.data, select = -c(names(corpus.data)[ncol(corpus.data)-1]))
ncol(corpus.data)-1
names(corpus.data)[ncol(corpus.data)-1]
corpus.data <- subset(corpus.data, select = -c(names(corpus.data)[ncol(corpus.data)-1]))
corpus.data <- subset(corpus.data, select = -c(v233))
corpus.data <- subset(corpus.data, select = -c("v233"))
corpus.data <- subset(corpus.data, select = -c(V233))
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, as.character(new.data$seller))
corpus.data <- as.data.frame(corpus.data)
names(corpus.data)[ncol(corpus.data)-1] <- "toDelete" #number of factor
names(corpus.data)[ncol(corpus.data)] <- "seller"
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, as.character(new.data$seller))
corpus.data <- as.data.frame(corpus.data)
names(corpus.data)[ncol(corpus.data)-1] <- "toDelete" #number of factor
names(corpus.data)[ncol(corpus.data)] <- "seller"
corpus.data[,234]
corpus.data[,233]
#install.packages("tm")
#install.packages("plyr")
#install.packages("class")
#install.packages("caret")
#install.packages("SnowBallC")
library(tm)
library(plyr)
library(class)
library(caret)
library(stringr)
data <- as.data.frame(read.csv("alphaClean.csv"))
# Select all "Drugs & Chemicals" ads
matching_vector <- c( str_detect(data$category, "Drugs & Chemicals"))
new.data <- data[matching_vector,]
# Handling : seller
tab_sel <- table(new.data$seller)
tab_sel <- sort(tab_sel, decreasing=TRUE)  # Sorting (biggest in first)
tab_sel <- tab_sel[1:5] # Taking only the most important : main sellers
name_sel <- names(tab_sel)
# New data keeping only the main sellers
new.data <-subset(new.data, seller %in% name_sel)
#new.data$seller <- factor(new.data$seller)
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, as.character(new.data$seller))
corpus.data <- as.data.frame(corpus.data)
corpus.data[,233]
names(corpus.data)[ncol(corpus.data)] <- "seller"
# Random rows :
corpus.data <- corpus.data[sample(nrow(corpus.data),nrow(corpus.data),replace=FALSE), ]
#train data
train <- corpus.data[1:round(nrow(corpus.data)*0.75,0),]
train <- as.data.frame(train)
#model
fit <- train(seller ~ ., data = train, method = 'bayesglm')
#test data
test <- as.matrix(tdm[(round(nrow(tdm)*3/4,0)+1), nrow(tdm),])
test <- as.data.frame(test)
test <- corpus.data[(round(nrow(corpus.data)*3/4,0)+1), nrow(corpus.data),]
round(nrow(corpus.data)*3/4,0)+1
nrow(corpus.data)
test <- corpus.data[(round(nrow(corpus.data)*3/4,0)+1), nrow(corpus.data),]
#test data
test <- corpus.data[(round(nrow(corpus.data)*0.75,0)+1): nrow(corpus.data),]
pred <- predict(fit, newdata = test)
pred
train$seller
corpus.data[,233]
train[,233]
names(train)[233]
train$seller
train[,233]
names(train)[233]
View(test)
View(test)
names(train)[233]
train$seller
names(corpus.data)[ncol(corpus.data)] <- "s"
#install.packages("tm")
#install.packages("plyr")
#install.packages("class")
#install.packages("caret")
#install.packages("SnowBallC")
library(tm)
library(plyr)
library(class)
library(caret)
library(stringr)
data <- as.data.frame(read.csv("alphaClean.csv"))
# Select all "Drugs & Chemicals" ads
matching_vector <- c( str_detect(data$category, "Drugs & Chemicals"))
new.data <- data[matching_vector,]
# Handling : seller
tab_sel <- table(new.data$seller)
tab_sel <- sort(tab_sel, decreasing=TRUE)  # Sorting (biggest in first)
tab_sel <- tab_sel[1:5] # Taking only the most important : main sellers
name_sel <- names(tab_sel)
# New data keeping only the main sellers
new.data <-subset(new.data, seller %in% name_sel)
#new.data$seller <- factor(new.data$seller)
#Create corpus
dataText <- subset(new.data, select = c(ad))
dataText.vec <- new.data$ad
corpus <- Corpus(VectorSource(dataText.vec))
#Preprocessing corpus
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, stripWhitespace)
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removeWords, stopwords("english"))
tdm = TermDocumentMatrix(corpus)
tdm = removeSparseTerms(tdm, 0.95)
#add sellers
corpus.data <- as.matrix(tdm)
corpus.data <- as.data.frame(corpus.data)
corpus.data <- t(corpus.data)
corpus.data <- cbind(corpus.data, as.character(new.data$seller))
corpus.data <- as.data.frame(corpus.data)
#names(corpus.data)[ncol(corpus.data)-1] <- "toDelete" #number of factor
names(corpus.data)[ncol(corpus.data)] <- "s"
#corpus.data <- subset(corpus.data, select = -c(V233))
#corpus.data <- subset(corpus.data, select = -c(toDelete))
#colnames(corpus.data)[ncol(corpus.data)] <- 'seller'
# Random rows :
corpus.data <- corpus.data[sample(nrow(corpus.data),nrow(corpus.data),replace=FALSE), ]
#train data
train <- corpus.data[1:round(nrow(corpus.data)*0.75,0),]
train <- as.data.frame(train)
names(train)[233]
train$s
train[,233]
#model
fit <- train(s ~ ., data = train, method = 'bayesglm')
#test data
test <- corpus.data[(round(nrow(corpus.data)*0.75,0)+1): nrow(corpus.data),]
pred <- predict(fit, newdata = test)
# Analysis:
# Comparison between the result and the prediction (prediction in colunm)
conf <- table(test[,match("s",names(test))],pred)
# Accurency :
acc <- sum(diag(conf)) / sum(conf)
print(conf)
print(acc)
train[,233]
train$s
install.packages("wordcloud")
#-----------------------------------------------
#       Words analysis
#-----------------------------------------------
#data <- as.data.frame(read.csv("../alphaClean.csv"))
library(stringr)
library(wordcloud)
# Pick up every words in column category and title
words.title.list <- str_split(data$title, boundary("word"))
#In category keep only the 2nd and 3rd sub-category
regex <- "/(.*)/(.*)/(.*)"
cat <- str_match(data$category, regex)
words.cat.vector <- c(cat[,3], cat[,4])
words.cat.vector <- unlist(str_split(words.cat.vector, boundary("word")))
#words.cat.list <- str_split(data$category, "/")
#word.cat.vector <- as.vector(unlist(words.cat.list))
#word.cat.vector <- sort(table(word.cat.vector), decreasing = TRUE)
words <- c(unlist(words.title.list), str_to_lower(words.cat.vector))
# calculate occurences
tableWords <- sort(table(words), decreasing = TRUE)
tableWords <- as.data.frame(tableWords)
tableWords$words <- as.character(tableWords$words)
# Removing conjonction, preposition, ??numbers??
commonWords <- read.csv("./More Stats/5000CommonWords.csv")
commonWords <- as.vector(commonWords$Word)
commonWords <- str_trim(commonWords)
tableWords <-subset(tableWords, !( words %in% commonWords))
tableWords <- tableWords[!str_detect(tableWords$words, "^[0-9.,]*$"),]
#Removing small frequences
tableWords <- tableWords[which(tableWords$Freq > nrow(data)/5000), ]
#Removing "number + unit"
# Vector with all the unit that are allowed (add unit if needed)
unit_allowed <- c("mg","kg", "ug","lb","oz","ounce","g","gr","gram")
# Construct a regular expression matching with digits + units allowed
regex_unit <- str_c("([0-9]+\\.?[0-9]*)((?:(\\s|)((",unit_allowed[1],")")
for(i in 2:length(unit_allowed)){
regex_unit <- str_c(regex_unit,"|(",unit_allowed[i],")")
}
regex_unit <- str_c(regex_unit,")))")
# regex_unit = regular expression for dose and unit
# Extraction from the title :
toRemove<- !str_detect(tableWords$words, regex_unit)
tableWords <- tableWords[toRemove,]
# Removing all words containing less than 2 letters
tableWords <- tableWords[str_count(tableWords[,1], "")>2,]
wordcloud(tableWords$words)
nrow(data)/100)
nrow(data)/100
tableWords <- tableWords[which(tableWords$Freq > nrow(data)/100), ]
wordcloud(tableWords$words)
#-----------------------------------------------
#       Words analysis
#-----------------------------------------------
#data <- as.data.frame(read.csv("../alphaClean.csv"))
library(stringr)
library(wordcloud)
# Pick up every words in column category and title
words.title.list <- str_split(data$title, boundary("word"))
#In category keep only the 2nd and 3rd sub-category
regex <- "/(.*)/(.*)/(.*)"
cat <- str_match(data$category, regex)
words.cat.vector <- c(cat[,3], cat[,4])
words.cat.vector <- unlist(str_split(words.cat.vector, boundary("word")))
#words.cat.list <- str_split(data$category, "/")
#word.cat.vector <- as.vector(unlist(words.cat.list))
#word.cat.vector <- sort(table(word.cat.vector), decreasing = TRUE)
words <- c(unlist(words.title.list), str_to_lower(words.cat.vector))
# calculate occurences
tableWords <- sort(table(words), decreasing = TRUE)
tableWords <- as.data.frame(tableWords)
tableWords$words <- as.character(tableWords$words)
# Removing conjonction, preposition, ??numbers??
commonWords <- read.csv("./More Stats/5000CommonWords.csv")
commonWords <- as.vector(commonWords$Word)
commonWords <- str_trim(commonWords)
tableWords <-subset(tableWords, !( words %in% commonWords))
tableWords <- tableWords[!str_detect(tableWords$words, "^[0-9.,]*$"),]
#Removing small frequences
tableWords <- tableWords[which(tableWords$Freq > nrow(data)/500), ]
#Removing "number + unit"
# Vector with all the unit that are allowed (add unit if needed)
unit_allowed <- c("mg","kg", "ug","lb","oz","ounce","g","gr","gram")
# Construct a regular expression matching with digits + units allowed
regex_unit <- str_c("([0-9]+\\.?[0-9]*)((?:(\\s|)((",unit_allowed[1],")")
for(i in 2:length(unit_allowed)){
regex_unit <- str_c(regex_unit,"|(",unit_allowed[i],")")
}
regex_unit <- str_c(regex_unit,")))")
# regex_unit = regular expression for dose and unit
# Extraction from the title :
toRemove<- !str_detect(tableWords$words, regex_unit)
tableWords <- tableWords[toRemove,]
# Removing all words containing less than 2 letters
tableWords <- tableWords[str_count(tableWords[,1], "")>2,]
wordcloud(tableWords$words)
warnings()
#-----------------------------------------------
#       Words analysis
#-----------------------------------------------
#data <- as.data.frame(read.csv("../alphaClean.csv"))
library(stringr)
library(wordcloud)
# Pick up every words in column category and title
words.title.list <- str_split(data$title, boundary("word"))
#In category keep only the 2nd and 3rd sub-category
regex <- "/(.*)/(.*)/(.*)"
cat <- str_match(data$category, regex)
words.cat.vector <- c(cat[,3], cat[,4])
words.cat.vector <- unlist(str_split(words.cat.vector, boundary("word")))
#words.cat.list <- str_split(data$category, "/")
#word.cat.vector <- as.vector(unlist(words.cat.list))
#word.cat.vector <- sort(table(word.cat.vector), decreasing = TRUE)
words <- c(unlist(words.title.list), str_to_lower(words.cat.vector))
# calculate occurences
tableWords <- sort(table(words), decreasing = TRUE)
tableWords <- as.data.frame(tableWords)
tableWords$words <- as.character(tableWords$words)
# Removing conjonction, preposition, ??numbers??
commonWords <- read.csv("./More Stats/5000CommonWords.csv")
commonWords <- as.vector(commonWords$Word)
commonWords <- str_trim(commonWords)
tableWords <-subset(tableWords, !( words %in% commonWords))
tableWords <- tableWords[!str_detect(tableWords$words, "^[0-9.,]*$"),]
#Removing small frequences
tableWords <- tableWords[which(tableWords$Freq > nrow(data)/500), ]
#Removing "number + unit"
# Vector with all the unit that are allowed (add unit if needed)
unit_allowed <- c("mg","kg", "ug","lb","oz","ounce","g","gr","gram")
# Construct a regular expression matching with digits + units allowed
regex_unit <- str_c("([0-9]+\\.?[0-9]*)((?:(\\s|)((",unit_allowed[1],")")
for(i in 2:length(unit_allowed)){
regex_unit <- str_c(regex_unit,"|(",unit_allowed[i],")")
}
regex_unit <- str_c(regex_unit,")))")
# regex_unit = regular expression for dose and unit
# Extraction from the title :
toRemove<- !str_detect(tableWords$words, regex_unit)
tableWords <- tableWords[toRemove,]
# Removing all words containing less than 2 letters
tableWords <- tableWords[str_count(tableWords[,1], "")>2,]
wordcloud(tableWords$words)
data <- as.data.frame(read.csv("../alphaClean.csv"))
library(stringr)
library(wordcloud)
# Pick up every words in column category and title
words.title.list <- str_split(data$title, boundary("word"))
#In category keep only the 2nd and 3rd sub-category
regex <- "/(.*)/(.*)/(.*)"
cat <- str_match(data$category, regex)
words.cat.vector <- c(cat[,3], cat[,4])
words.cat.vector <- unlist(str_split(words.cat.vector, boundary("word")))
#words.cat.list <- str_split(data$category, "/")
#word.cat.vector <- as.vector(unlist(words.cat.list))
#word.cat.vector <- sort(table(word.cat.vector), decreasing = TRUE)
words <- c(unlist(words.title.list), str_to_lower(words.cat.vector))
# calculate occurences
tableWords <- sort(table(words), decreasing = TRUE)
tableWords <- as.data.frame(tableWords)
tableWords$words <- as.character(tableWords$words)
# Removing conjonction, preposition, ??numbers??
commonWords <- read.csv("./More Stats/5000CommonWords.csv")
commonWords <- as.vector(commonWords$Word)
commonWords <- str_trim(commonWords)
tableWords <-subset(tableWords, !( words %in% commonWords))
tableWords <- tableWords[!str_detect(tableWords$words, "^[0-9.,]*$"),]
#Removing small frequences
tableWords <- tableWords[which(tableWords$Freq > nrow(data)/500), ]
#Removing "number + unit"
# Vector with all the unit that are allowed (add unit if needed)
unit_allowed <- c("mg","kg", "ug","lb","oz","ounce","g","gr","gram")
# Construct a regular expression matching with digits + units allowed
regex_unit <- str_c("([0-9]+\\.?[0-9]*)((?:(\\s|)((",unit_allowed[1],")")
for(i in 2:length(unit_allowed)){
regex_unit <- str_c(regex_unit,"|(",unit_allowed[i],")")
}
regex_unit <- str_c(regex_unit,")))")
# regex_unit = regular expression for dose and unit
# Extraction from the title :
toRemove<- !str_detect(tableWords$words, regex_unit)
tableWords <- tableWords[toRemove,]
# Removing all words containing less than 2 letters
tableWords <- tableWords[str_count(tableWords[,1], "")>2,]
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
setwd("C:/Internship/UEL-project")
source('C:/Internship/UEL-project/More stats/analysisWords.R', echo=TRUE)
set.seed(142)
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
set.seed(142)
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
wordcloud(tableWords$words, tableWords$Freq, colors=brewer.pal(6,"Dark2"))
View(tableWords)
View(tableWords)
wordcloud(tableWords$words, tableWords$Freq, min.freq = tableWords$Freq[100], colors=brewer.pal(6,"Dark2"))
wordcloud(tableWords$words, tableWords$Freq, min.freq = tableWords$Freq[100], colors=brewer.pal(6,"Dark2"))
wordcloud(tableWords$words, tableWords$Freq, min.freq = tableWords$Freq[100], colors=brewer.pal(6,"Dark2"))
source('C:/Internship/UEL-project/More stats/analysisWords.R', echo=TRUE)
View(tableWords)
